---
title: 'Assignment 3 Question 2 MAST90125: Steven Maharaj 695281'
header-includes:
   - \usepackage{bm}
   - \usepackage{amsmath}
   - \usepackage{amssymb}
output: 
  pdf_document:
    number_sections: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Due: Friday 25 October 2019**  
\vspace{5 mm}

**There are places in this assignment where R code will be required. Therefore set the random seed so assignment is reproducible.**

```{r}
set.seed(695281)  #Please change random seed to your student id number.
library(mvtnorm)
```


## Question Two: (13 marks)

Researchers are interested in phone call duration. The available data consisted of 100 standardised phone call durations, and which hour ($t = 1, \ldots, 10$) the call was initiated. The researchers assumed the following model,

\begin{eqnarray}
p(y_i | \mu(t)) &=& \mathcal{N}(\mu(t),\sigma^2) \nonumber \\
p(\mu(t))   &=& \mathcal{N}(0,k(t,t))        \nonumber 
\end{eqnarray}

such that the covariance function $k(x,x')$ is squared exponential,

\[ k(x,x') = \sigma^2_Ke^{-l\times (x-x')^2},   \]

with $\sigma^2_K$ fixed to 1.85 and $l$ fixed to $0.05$. The data can be downloaded from LMS as \texttt{call.csv}.

a) The researchers are interested in making predictions of phone call duration, $\tilde{\bm \mu}(t)$ for hours $t = 1, \ldots, 12$. As an initial step, they assumed $\sigma^2$ was 0.95. Assume the data $\bf y$ is conditioned on a single realisation of the Gaussian process prior. Do the following:

\begin{itemize}
\item Based on the information provided, determine the joint distribution of data $\bf y$ and predictions $\tilde{\bm \mu}(t)$.
\item Determine the distribution of $\tilde{\bm \mu}(t)$ conditional on $\bf y$, $\sigma^2$, $\sigma^2_K$ and $l$. Show working
\item Plot the predictions with 95 \% and 99 \% credible intervals along with the observed data. Comment, in a Bayesian language, on the behaviour of predictions where no data was observed.
\end{itemize}




Answer:
The joint distribution of the data $\bm y$ and predictions is $\mu(\tilde{\mathbf{x}})$ is
For lecture it was shown that
$$p\left(\begin{array}{c}{\mathbf{y}} \\ {\mu(\tilde{\mathbf{x}})}\end{array}\right)=\mathcal{N}\left(\left(\begin{array}{c}{\boldsymbol{m}(\mathbf{x})} \\ {\boldsymbol{m}(\tilde{\mathbf{x}})}\end{array}\right),\left(\begin{array}{cc}{\boldsymbol{k}(\mathbf{x}, \mathbf{x})+\boldsymbol{\Sigma}} & {\boldsymbol{k}(\mathbf{x}, \tilde{\mathbf{x}})} \\ {\boldsymbol{k}(\tilde{\mathbf{x}}, \mathbf{x})} & {\boldsymbol{k}(\tilde{\mathbf{x}}, \tilde{\mathbf{x}})}\end{array}\right)\right)$$

For this question joint distribution of the data $\mathbf{y}$ and predictions is $\mu(\tilde{\mathbf{t}})$ is

For lecture it was shown that
$$p\left(\begin{array}{c}{\mathbf{y}} \\ {\mu(\tilde{\mathbf{t}})}\end{array}\right)=\mathcal{N}\left(\left(\begin{array}{c}{\boldsymbol{0}} \\ {\boldsymbol{0}}\end{array}\right),\left(\begin{array}{cc}{\boldsymbol{k}(\mathbf{t}, \mathbf{t})+\boldsymbol{\Sigma}} & {\boldsymbol{k}(\mathbf{t}, \tilde{\mathbf{t}})} \\ {\boldsymbol{k}(\tilde{\mathbf{t}}, \mathbf{t})} & {\boldsymbol{k}(\tilde{\mathbf{t}}, \tilde{\mathbf{t}})}\end{array}\right)\right)$$
where $k\left(t_i, \tilde{t_j}\right)=\sigma_{K}^{2} e^{-l \times\left(t_i-\tilde{t_j}\right)^{2}}$ and $\Sigma = \sigma^2 \bm I$

Now we will find $\mu(\tilde{\mathbf{t}})$ conditional on $\mathbf{y}, \sigma^{2}, \sigma_{K}^{2}, l$.

For lectures, the noisless case had the follow distribution.

$$\left.\boldsymbol{\mu}(\tilde{\mathbf{t}}) | . \sim \mathcal{N}(\boldsymbol{m}(\tilde{\mathbf{t}})+\boldsymbol{k}(\tilde{\mathbf{t}}, \mathbf{t}) \boldsymbol{k}(\mathbf{t}, \mathbf{t})-\mathbf{m}(\mathbf{t})), \boldsymbol{k}(\tilde{\mathbf{t}}, \tilde{\mathbf{t}})-\boldsymbol{k}(\tilde{\mathbf{t}}, \mathbf{t}) \boldsymbol{k}(\mathbf{t}, \mathbf{t})^{-1} \boldsymbol{k}(\mathbf{t}, \tilde{\mathbf{t}})\right)$$
for this question  $\boldsymbol{m}(\tilde{\mathbf{t}}) = \boldsymbol{0}$, we must add $\Sigma$ to the first element of covariance block matrix. In other words $\boldsymbol{k}(\mathbf{t}, \mathbf{t}) \rightarrow \boldsymbol{k}(\mathbf{t}, \mathbf{t}) +\Sigma$. Then we have that following

$$\boldsymbol{\mu}(\tilde{\mathbf{t}}) |\mathbf{y}, \sigma^{2}, \sigma_{K}^{2} , l \sim \mathcal{N}\left(\boldsymbol{k}(\tilde{\mathbf{t}}, \mathbf{t}) \left[\boldsymbol{k}(\mathbf{t}, \mathbf{t}) +\Sigma \right]^{-1}\mathbf{y}, \boldsymbol{k}(\tilde{\mathbf{t}}, \tilde{\mathbf{t}})-\boldsymbol{k}(\tilde{\mathbf{t}}, \mathbf{t}) \left[\boldsymbol{k}(\mathbf{t}, \mathbf{t}) +\Sigma \right]^{-1} \boldsymbol{k}(\mathbf{t}, \tilde{\mathbf{t}})\right)$$
```{r}
df <- read.csv("call.csv")
```

```{r}
mu.fun<-function(y,t,l,sigma2,sigmak2,n){

  np<-length(t) #number of location to evaluate Gaussian process.
  mT <- matrix(df$hour,np,np)
  mT_hat<-matrix(t,np,np)
  Sigma <- sigma2*diag(np)
  # kt_t<- sigmak2*exp(-l*(mT-t(mT))) + Sigma
  kt_t<- Sigma
  kt_th<- sigmak2*exp(-l*(mT-mT_hat)^2)
  kth_th<- solve(Sigma)
  result<-rmvnorm(n,mean=t(kt_th)%*% solve(kt_t)%*%y,sigma= kth_th - ( t(kt_th)%*% solve(kt_t)%*%kt_th ) )
  return(result)
}

#An example of generating function with $n=5$.
t<-sort(sample(1:12,100,replace = TRUE)) #generate 100 points for gaussian process to be evaluated at.
test<-mu.fun(y=df$duration,t=t,l=0.05,sigma2=0.95,sigmak2=1.85,n=1)#plotting result
test_std <- (test-mean(test))/sd(test)
plot(density(test_std),main='realisations of Gaussian process', xlab="t",ylab="standardised duration")
den <- density(df$duration)
lines(density(df$duration),col=2)
abline(v=quantile(test_std,c(0.025,0.975)),col=3)
abline(v=quantile(test_std,c(0.005,0.995)),col=4)
legend("topright",legend = c("prediction","observed","95CI","99CI"),col = 1:4,lty = 1)
```


We see that on the right side of the distribution the upper limits are further from zero than lower limits. This is because there is more uncertainty at times we did not observe(t =11,12). 


b) Was the inference performed in part a) fully Bayesian? If not, how would you make the analysis fully Bayesian, noting any particular difficulties that arise by assuming a Gaussian process prior. Your answer should not exceed one page of writing.


Answer:

The inference from part a) was not fully bayesian as we fixed $\sigma^{2}, \sigma_{K}^{2} , l$. In order to make the inference fully bayesian one should assign priors to $\sigma^{2}, \sigma_{K}^{2} , l$.

A difficulity of using a Gaussian prior is that $\bm K(t,t)$ will not be full rank for $\bm t_i = \bm t_j$ for every element this would mean $\bm K(t,t)$ is not invertiable.
