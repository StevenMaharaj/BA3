---
title: 'Assignment 3 Question 1 MAST90125: Bayesian Statistical Learning'
header-includes:
   - \usepackage{bm}
   - \usepackage{amsmath}
output: 
  pdf_document:
    number_sections: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Due: Friday 25 October 2019**  
\vspace{5 mm}

**There are places in this assignment where R code will be required. Therefore set the random seed so assignment is reproducible.**

```{r}
set.seed(695281) 
library(mvtnorm) #Please change random seed to your student id number.
```


```{r}
# Read  data
WOOL <- read.csv("Warpbreaks.csv")
```

```{r}
# model poisson regression 
mod<-glm(breaks~ ., WOOL, family = poisson(link = "log"))
X <- model.matrix(mod)
# sigma  <-vcov(mod)
y <- WOOL$breaks
p <-dim(X)[2]   #number of parameters
M <- 5*crossprod(X)
```

```{r}

#Part one: function for performing Hamiltonian Monte Carlo for logistic regression.
#Inputs:
#y: vector of responses
#n: vector (or scalar) of trial sizes. 
#X: predictor matrix including intercept.
#L: number of leapfrog steps.
#M is variance covariance matrix for normal prior of momentum variable \phi. Ideally diagonal.
#iter: number of iterations
#burnin: number of initial iterations to throw out.
HMC.fn<-function(y,X,L,M,iter,burnin){ 
p <-dim(X)[2]   #number of parameters
library(mvtnorm)
theta0<-rnorm(p) #initial values of beta
theta.sim<-matrix(0,iter,p+1) #matrix to store iterations plus acceptance.
theta.sim[1,1:p]<-theta0       #initial values in matrix.
epsilon<-1/L                #epsilon assuming epsilon*L =1.
Minv   <-solve(M)


for(i in 1:(iter-1)){
phi       <-rmvnorm(1,mean=rep(0,p),sigma=M)   #draw momentum variable.
phi       <-as.numeric(phi)
phi0      <-phi                             #saving starting phi for calculation of r. 
theta     <-theta.sim[i,1:p]                 #current state of theta.  

lbd.b         <-exp(X%*%theta)  #calculate lambda.
gradtheta <- crossprod(X,y-lbd.b )   #Gradient of posterior = joint distribution with respect to theta.

#leapfrog steps.
for(j in 1:L){
  phi   <- phi + 0.5*epsilon*gradtheta   #first half step for phi
  theta <- theta + epsilon*(Minv%*%phi)  #full step for theta
  
lbd.c         <-exp(X%*%theta) #calculate probabilities of success at candidate (sub) state.
gradtheta <- crossprod(X,y-lbd.c )   #Gradient of posterior = joint distribution with respect to theta.

phi   <- phi + 0.5*epsilon*gradtheta #second half step for phi.
phi   <- as.numeric(phi)
}

#difference of log joint distributions at final iteration of leap.frog vs current state.
r<-sum( dpois(y,lambda = lbd.c,log=TRUE))+dmvnorm(phi,mean=rep(0,p),sigma=M,log=TRUE)-sum(dpois(y,lambda = lbd.b,log=TRUE) )-dmvnorm(phi0,mean=rep(0,p),sigma=M,log=TRUE)
#Draw an indicator whether to accept/reject candidate
ind<-rbinom(1,1,exp( min(c(r,0)) ) )
theta.sim[i+1,1:p]<- ind*theta + (1-ind)*theta.sim[i,1:p]
theta.sim[i+1,p+1] <- ind
}

#Removing the iterations in burnin phase
results<-theta.sim[-c(1:burnin),]
names(results)<-c('beta0','beta1','beta2','beta3','accept') #column names

return(results)
}
```

```{r}
# L = 2
HMCl2<-HMC.fn(y=y,X=X,L=2,M=M,iter=10000,burnin=3000)

# L = 3
HMCl3<-HMC.fn(y=y,X=X,L=3,M=M,iter=10000,burnin=3000)

# L = 4
HMCl4<-HMC.fn(y=y,X=X,L=4,M=M,iter=10000,burnin=3000)

```


For L = 2
```{r}
#Posterior means of beta0, beta1, beta2, beta3 Acceptance rate comparison

colMeans(HMCl2)

#Posterior standard deviations
apply(HMCl2,2,FUN=sd)

#90 % credible intervals  
apply(HMCl2,2,FUN=function(x) quantile(x,c(0.05,0.95)) )

# acceptance rate 
colMeans(HMCl2)[5]
```


For L = 3
```{r}
#Posterior means of beta0, beta1, beta2, beta3 Acceptance rate comparison

colMeans(HMCl3)

#Posterior standard deviations
apply(HMCl3,2,FUN=sd)

#90 % credible intervals  
apply(HMCl3,2,FUN=function(x) quantile(x,c(0.05,0.95)) )

# acceptance rate 
colMeans(HMCl3)[5]

```

For L = 4
```{r}
#Posterior means of beta0, beta1, beta2, beta3 Acceptance rate comparison
colMeans(HMCl4)

#Posterior standard deviations
apply(HMCl4,2,FUN=sd)

#90 % credible intervals  
apply(HMCl4,2,FUN=function(x) quantile(x,c(0.05,0.95)) )

# acceptance rate 
colMeans(HMCl4)[5]
```

The 90% credible interval tells there is 90% probability that parameter we seek to estimate is between the lower and upper bound of the given interval.

____________________________
part D

Answer: 
We will visually check if the chains converged to the same distribution

```{r}
#plotting HMC.
par(mfrow=c(2,2))
for (i in 1:4) {
plot(HMCl2[,i],type='l',main='Hamiltonian MC output',xlab='Iteration',ylab=bquote( beta[.(i-1)] ))
lines(HMCl3[,i],col=2)
lines(HMCl4[,i],col=3)
}

```

Checking the Gelman-Rubin diagnostic.

```{r}
library(coda)
hl1<-as.mcmc.list(as.mcmc((HMCl2[1:3500,1:4])))
hl2<-as.mcmc.list(as.mcmc((HMCl2[1:3500,1:4])))
hl3<-as.mcmc.list(as.mcmc((HMCl3[1:3500,1:4])))
hl4<-as.mcmc.list(as.mcmc((HMCl3[3500+1:3500,1:4])))
hl5<-as.mcmc.list(as.mcmc((HMCl4[3500+1:3500,1:4])))
hl6<-as.mcmc.list(as.mcmc((HMCl4[3500+1:3500,1:4])))
hl<-c(hl1,hl2,hl3,hl4,hl5,hl6)

#Gelman-Rubin diagnostic.
gelman.diag(hl)[[1]]
#effective sample size.
effectiveSize(hl)


```
From ploting the iterations and computing the Gelman-Rubin diagnostic we see that all the chains converged to the same distribution.

Now let us check the autocorrelation in each chain using acf plots.
```{r}
# L=2
par(mfrow=c(2,2))
for (i in 1:4) {
acf(HMCl2[,i],ylab=bquote( beta[.(i-1)] ),lag.max = 15,main="L = 2")
}
# L=3
par(mfrow=c(2,2))
for (i in 1:4) {
acf(HMCl3[,i],ylab=bquote( beta[.(i-1)] ),lag.max = 15,main="L = 3")
}
# L=4
par(mfrow=c(2,2))
for (i in 1:4) {
acf(HMCl4[,i],ylab=bquote( beta[.(i-1)] ),lag.max = 15,main="L = 4")
}
```

So from above we observe that obseve that as L increase, the acceptance rate increase and from the acf plot the autocorrelation also increases. Thus one should choose a moderate value of L. In our case one should choose L=3.  
